\documentclass[a4paper]{article}

\usepackage{listings}
\usepackage{amsthm}
\usepackage{mathtools}

\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclareMathOperator{\length}{length}
\DeclareMathOperator{\rnd}{rnd}
\DeclareMathOperator{\fsqrt}{fsqrt}
\DeclareMathOperator{\fsqr}{fsqr}
\DeclareMathOperator{\float}{float}
\DeclareMathOperator{\rint}{rint}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\title{Python's integer square root algorithm}
\author{Mark Dickinson}

\begin{document}
\lstset{language=Python}
\maketitle
\begin{abstract}
We present an adaptive-precision variant of Heron's method for computing
integer square roots of arbitrary-precision integers. The method is efficient
both at small scales and asymptotically, and represents an attractive
compromise between speed and simplicity. Since Python 3.8, the algorithm is
used by the CPython implementation of the Python programming language for its
standard library integer square root function.
\end{abstract}
\section{Introduction}

We start with a simple definition.

\begin{definition}
  For a nonnegative integer $n$, the \emph{integer square root} of $n$ is
  the unique nonnegative integer $a$ satifying $a^2 \le n < (a + 1)^2$.
\end{definition}

Equivalently, the integer square root of $n$ is simply the integer part
of the exact square root of $n$, or $\floor{\sqrt n}$.

The integer square root is a basic building block of any arbitrary-precision
arithmetic toolkit. Many number-theoretic and cryptographic algorithms require
the ability to detect whether a given integer is a square, and if so, to
extract its root.

For small $n$, it's feasible to use floating-point arithmetic to
compute integer square roots. For example, assuming IEEE 754 binary64 format
floating-point and a correctly-rounded square root operation, one can show that
computing $\floor{\sqrt n}$ directly gives the integer square root of $n$,
provided that $n < 2^{52} + 2^{27}$. However, neither the Python language nor
the CPython reference implementation of that language provides a guarantee of
either IEEE 754 format floating-point or correct rounding of floating-point
arithmetic operations. As such, any floating-point-based method for computing
an integer square root would need a correctness check along with a pure-integer
fallback method for the case where the floating-point square root operation
fails to provide sufficient accuracy. To avoid this complication, and to allow
integer square roots to be computed for arbitrarily large integers, it's
desirable to have an integer square root algorithm that works entirely with
integer arithmetic.

In this article we present a simple and fast pure-integer algorithm to compute
the integer square root of an arbitrary positive integer. This algorithm has
been implemented for CPython's \lstinline{math} module and is available from
Python 3.8 onwards as \lstinline{math.isqrt}.

Section~\ref{old_method} of this article reviews a well-known method for
computing integer square roots based on Heron's method. This provides much of
the background we need to introduce our algorithm, which is presented in
section~\ref{new_method}.

\section{Heron's method}
\label{old_method}

In this section we describe a well-known approach to computing integer square
roots, based on an integer-arithmetic version of Heron's method (also known
as the Babylonian method, and expressible as a special case of the
Newton--Raphson root-finding method). This is the approach
used for example by Java's \lstinline$BigInteger$ class.

Heron's method is based on the observation that if $x$ is a real approximation
to the square root of a positive real number $n$, then
$$\frac{x + n/x}2$$ is an improved approximation. Iterating then allows the
square root to be computed to any desired accuracy. It can be shown that the
method converges towards the square root from any positive starting
approximation $x$, and that once $x$ gets close to the true square root the
convergence is quadratic, so that the number of correct decimal places roughly
doubles with each iteration.

\begin{example}
  Given an approximation $x = 7/5 = 1.4$ to the square root $1.414213562\dots$
  of $2$, a single iteration of Heron's method produces a new approximation
  $(7/5 + 2/(7/5))/2 = 99/70 = 1.414285714\dots$. Applying a second iteration
  with $99/70$ as input gives $19601/13860 = 1.414213564\dots$, which is
  accurate to $8$ decimal places. A third iteration gives a value accurate to
  $17$ decimal places.
\end{example}

Heron's method can be adapted to the domain of positive integers. For a fixed
positive integer $n$, define a function $f$ on the positive integers by
$$f(a) = \floor*{\frac{a + \floor{n/a}}2}.$$

The idea is that---just like its real counterpart---the function $f$ should
transform a poor approximation to the integer square root of $n$ into a better
one, and so by applying $f$ repeatedly we should eventually reach the integer
square root. This works, but we have to be a little careful with the details.
In particular, deciding when to stop iterating is delicate. The lemmas below
make this precise.

\begin{lemma}
  \label{heron_high}
  For any positive integer $a$, $f(a)$ is greater than or equal to the
  integer square root of $n$.
\end{lemma}

\begin{proof}
  The AM-GM inequality applied to $a$ and $n/a$ gives
  $$\sqrt n \le \frac{a + n/a}2.$$
  Hence
  $$\floor{\sqrt n} \le \floor*{\frac{a + n/a}2}
  = \floor*{\frac{\floor{a + n/a}}2}
  = \floor*{\frac{a + \floor{n/a}}2} = f(a).$$
\end{proof}

\begin{lemma}
  \label{heron_decreases}
  Given a positive integer $a$ greater than the integer square
  root of $n$, $f(a)$ is smaller than $a$.
\end{lemma}

\begin{proof}
  We have the following chain of equivalences:
  \begin{align*}
    \floor{\sqrt n} < a &\iff \sqrt n < a \\
                        &\iff n < a^2 \\
                        &\iff n/a < a \\
                        &\iff \floor{n/a} < a \\
                        &\iff a + \floor{n/a} < 2a \\
                        &\iff \frac{a + \floor{n/a}}2 < a \\
                        &\iff \floor*{\frac{a + \floor{n/a}}2} < a \\
                        &\iff f(a) < a.
  \end{align*}
  This completes the proof.
\end{proof}

Now suppose that we're given a starting guess $a$ that exceeds the integer
square root of $n$: $\floor{\sqrt n} < a$. Combining the above lemmas,
we have
$$\floor{\sqrt n} \le f(a) < a.$$ If $f(a)$ is again greater than the integer
square root of $n$, we have:
$$\floor{\sqrt n} \le f(f(a)) < f(a) < a$$ and so on. So the sequence
$$a, f(a), f(f(a)), f(f(f(a))), \dots$$
must eventually reach the integer square root. If not, we'd have an infinite
strictly decreasing sequence of positive integers, violating the well-ordering
principle.

Lemma \ref{heron_decreases} also provides a way to detect \emph{when}
we've reached the integer square root: if $\floor{\sqrt n} \le a$ but the
inequality $f(a) < a$ fails, then $a$ cannot be greater than the integer square
root, so it must \emph{be} the integer square root. However, we need to be
careful. While it's true that the sequence $a, f(a), f(f(a)), \dots$ must
eventually reach the square root, it would be a mistake to claim that it
\emph{converges} to the square root: we don't necessarily always reach a point
where $f(a) = a$. For example, if $n=15$, the sequence of iterates of $f$ will
eventually alternate between $3$ and $4$, and more generally this sort of
alternation will occur for any $n$ of the form $a^2 - 1$ for some $a \ge 2$.

Listing~\ref{direct_heron} expresses the algorithm described above in Python. A
linguistic note: Python's \lstinline$//$ operator does ``floor division'':
\lstinline$n//a$ represents $\floor{n / a}$. Indeed, from a computational
perspective, an expression like $\floor{n / a}$ is misleading: it resembles a
composition of two operations, while most programming languages or
arbitrary-precision integer-arithmetic packages will provide some form of
integer division as an integer-to-integer primitive.

\lstinputlisting[
  float,
  label=direct_heron,
  frame=single,
  caption={Integer square root via Heron's method, version 1},
  lastline=12]{isqrt_newton_direct.py}

The code in Listing~\ref{direct_heron} uses $n$ as the initial estimate. That's
highly inefficient for large $n$: the initial iterations will roughly halve the
estimate each time, and many iterations will be needed to get into the general
neighborhood of the square root. We can do better: \emph{any} positive integer
$a \ge \floor{\sqrt n}$ can be used as a starting value. Alternatively, if we
have a starting guess $a$ that we know is close to the square root, but we
don't know for sure that $a \ge \floor{\sqrt n}$, we can apply a single
iteration to replace $a$ with $f(a)$ before running the main algorithm; by
Lemma~\ref{heron_high}, $f(a)$ is guaranteed to be no smaller than the integer
square root.

One simple, easy-to-compute possibility that's not too inefficient is to take
$a$ to be the smallest power of two exceeding $\floor{\sqrt n}$. Before giving
the code, we make a definition.

\begin{definition}
  For a nonnegative integer $n$, the \emph{bit length} of $n$, written
  $\length(n)$, is the least nonnegative integer $e$ for which $n < 2^e$.
\end{definition}

For positive $n$ the bit length of $n$ is $1 + \floor{\log_2(n)}$, while the
bit length of $0$ is $0$. For nonnegative integers $n$ and $e$, we have $2^e
\le n$ if and only if $e < \length(n)$, and $\length(n) \le e$ if and only if $n
< 2^e$. In Python, the bit length of a nonnegative integer \lstinline$n$ can be
computed as \lstinline$n.bit_length()$.

Given nonnegative integers $n$ and $e$, we have the following chain
of equivalences:
\begin{align*}
  \floor{\sqrt n} < 2^e
    &\iff \sqrt n < 2^e \\
    &\iff n < 2^{2e} \\
    &\iff \length(n) \le 2e \\
    &\iff \length(n) < 2e + 1 \\
    &\iff \floor{(\length(n) - 1) / 2} < e \\
    &\iff \floor{(\length(n) + 1) / 2} \le e
\end{align*}
So taking $e = \floor{(\length(n) + 1) / 2}$, $2^e$ is the smallest power of
two that strictly exceeds the square root of~$n$. Using that as our starting
guess, and taking the opportunity to streamline the previous code a little at
the same time, we get the algorithm in Listing~\ref{heron_improved}, in which
the termination condition $f(a) \ge a$ has been replaced with the equivalent
condition $\floor{n/a} \ge a$.

\lstinputlisting[
  float,
  label={heron_improved},
  frame=single,
  caption={Integer square root via Heron's method, streamlined},
  lastline=7]{isqrt_newton.py}

Note: since we actually only need $\floor{\sqrt n} \le 2^e$ rather than
$\floor{\sqrt n} < 2^e$, we could tighten our initial bound slightly by using
$\length(n-1)$ instead of $\length(n)$. But that costs an extra operation for
all inputs $n$, for a benefit only in the rare case that $n$ is an exact power
of four. As such, the change is probably not worth it.

The algorithm above is well-known and well-used. It has the virtue of
simplicity, and is reasonably efficient for inputs that aren't too large.
Nevertheless, there's room for improvement. With the current initial guess, for
large inputs (say a few thousand bits or more), the first few iterations of the
algorithm are performing expensive full-precision divisions to obtain only a
handful of new correct bits at each iteration. The initial guess could perhaps
be improved at the expense of some additional complexity. There's potential
inefficiency towards the end of the algorithm, too. To demonstrate this last
point, consider the following example.

\begin{example}
  Take $n = 16785408$, which is just a little larger than $2^{24} = 16777216$.
  Our starting guess for the square root is $2^{13} = 8192$. Successive
  iterations give $5120$, $4199$, $4098$, $4097$, and finally $4096$, which
  is the integer square root. Each iteration requires one division, and
  a final division is needed to establish the termination condition, for
  a total of six divisions.
\end{example}

So even starting from $4098$, just a distance of two away from the true integer
square root, three more divisions are required before the correct integer
square root can be returned. (Note that this example is representative of the
worst-case behaviour rather than the typical behaviour of the algorithm.)

The algorithm introduced in the next section addresses these deficiencies,
while retaining much of the simplicity of the algorithm in this section.

\section{Variable-precision Heron's method}
\label{new_method}

In this section we introduce a simple variant of Heron's method that's
significantly more efficient than the basic algorithm for large inputs. As in
the previous section, we assume that $n$ is a positive integer, and we aim to
compute the integer square root of $n$.

There are two key ideas. First, we vary the precision as we go: our algorithm
produces at each iteration an integer approximation to the square root of
$\floor{n/4^f}$ for some integer $f$, with $f$ decreasing to $0$ as the
iterations progress. Second, we don't insist on obtaining the exact integer
square root at each iteration (which would require a per-iteration
check-and-correct step), but instead allow the error to propagate, and we prove
that with careful control of the precision increases, the error remains bounded
throughout the algorithm. We then use a single check-and-correct step
at the end of the algorithm.

To describe the algorithm, it's convenient to introduce the notion of a
``near square root".

\begin{definition}
  Suppose $n$ is a positive integer. Call a positive integer $a$ a
  \emph{near square root} of $n$ if $(a - 1)^2 < n < (a + 1)^2$.
\end{definition}

In other words, $a$ is a near square root of $n$ if $a$ is either $\floor{\sqrt
n}$ or $\ceil{\sqrt n}$. In particular, if $n = a^2$ is a perfect square then
$a$ is the \emph{only} near square root of $n$.

Given a near square root $a$ of $n$, the integer square root of $n$ is clearly
either $a$ or $a-1$, depending on whether $a^2 \le n$ or $a^2 > n$
(respectively). So an algorithm for computing near square roots provides us
with a way to compute integer square roots, and in the remainder of this
section we focus on finding near square roots.

Our algorithm is based on the idea of ``lifting" a near square root of a
quotient of $n$ to a near square root of $n$. Suppose that $j$ is a positive
integer and $b$ is a near square root of $\floor{n / j^2}$. Then $b$ is an
approximation to $\sqrt n / j$, and so $jb$ is an approximation to $\sqrt n$. A
single iteration of the integer form of Heron's method applied to $jb$ then
gives an improved approximation. If $j$ is not too large with respect to $n$,
this improved approximation will again be a near square root of~$n$.

Here's a theorem that makes that ``not too large" bound precise. For
simplicity, we restrict~$j$ to be an even integer. Write $j = 2k$, then in the
above discussion, $b$ is a near square root of~$\floor{n / 4k^2}$, $2kb$ is an
approximation to $\sqrt n$, and our improved approximation is $kb + \floor{n /
4kb}$.

\begin{theorem}\label{main_theorem}
  Suppose that $n \ge 4$ and $k$ is a positive integer satisfying $4k^4 \le
  n$. Let $b$ be a near square root of $\floor{n / 4k^2}$. Then the positive
  integer $a$ defined by
  $$ a = kb + \floor*{\frac{n}{4kb}}. $$
  is a near square root of $n$.
\end{theorem}

\begin{proof}
  By definition of near square root, we have
  \begin{equation}\label{b_nsqrt}
    (b - 1)^2 < \floor*{\frac{n}{4k^2}} < (b + 1)^2.
  \end{equation}
  Since $(b + 1)^2$ is an integer, we can remove the floor brackets to obtain
  \begin{equation}\label{b_nsqrt_weak}
    (b - 1)^2 < \frac{n}{4k^2} < (b + 1)^2.
  \end{equation}
  Multiplying by $4k^2$ throughout \eqref{b_nsqrt_weak} and then taking square
  roots gives
  \begin{equation}
    2k(b - 1) < \sqrt n < 2k(b + 1)
  \end{equation}
  which can be rearranged to the equivalent statement
  \begin{equation}
    \abs{2kb - \sqrt n} < 2k.
  \end{equation}
  Squaring and then dividing through by $4kb$ gives
  \begin{equation}
    0 \le kb + \frac{n}{4kb} - \sqrt n < k/b,
  \end{equation}
  which implies that
  \begin{equation}
    -1 < kb + \floor*{\frac{n}{4kb}} - \sqrt n < k/b.
  \end{equation}
  Substituting the definition of $a$ gives
  \begin{equation}\label{a_close_to_sqrt_n}
    -1 < a - \sqrt n < k/b.
  \end{equation}
  To complete the proof, we need to know that $k / b \le 1$. From our (not yet
  used) assumption that $4k^4 \le n$ we have $k^2 \le n / 4k^2$, while from the
  right-hand side of \eqref{b_nsqrt_weak} we have $n / 4k^2 < (b + 1)^2$.
  Combining these and taking square roots, $k < b + 1$, hence $k \le b $ and $k
  / b \le 1$. So now combining this with \eqref{a_close_to_sqrt_n} gives
  \begin{equation}
    -1 < a - \sqrt n < 1
  \end{equation}
  from which $(a - 1)^2 < n < (a + 1)^2$, so $a$ is a near square of~$n$, as
  required.
\end{proof}

\begin{example}
  Let $n = 46696$ and $k=10$. Then $\floor{n / k^2} = 116$, so $10$ and
  $11$ are both near square roots of $\floor{n / k^2}$.

  Taking $b = 10$, we get $a = 100 + \floor{46696 / 400} = 216$. Since
  $\sqrt{46696} = 216.092572755\dots$, $216$ is indeed a near square root of
  $n$. If we take $b = 11$, we also get $a = 110 + \floor{46696 / 440} = 216$.
\end{example}

Now we turn to implementation. Like many arbitrary-precision integer
implementations, Python's integer implementation is based on binary, so
multiplications and integer divisions by powers of two can be performed
efficiently by bit-shifting. So when applying Theorem~\ref{main_theorem}, we'll
choose our $k$ to be the largest power of two satisfying $4k^4 \le n$. Writing
$k=2^e$, we have:
\begin{align*}
  4k^4 \le n
  &\iff 2^{2 + 4e} \le n \\
  &\iff 2 + 4e < \length n \\
  &\iff 3 + 4e \le \length n \\
  &\iff e \le \floor*{\frac{\length n - 3}{4}}
\end{align*}
So we take $k = 2^e$, where $e={\floor{(\length n - 3)/4}}$. This gives the recursive
near square root implementation shown in Listing~\ref{rec}, where the
multiplication by $k$ and the divisions by $4k$ and $4k^2$ are replaced by the
corresponding bit-shift operations.

\lstinputlisting[
  float,
  frame=single,
  label={rec},
  caption={Adaptive precision Heron's method, recursive},
  lastline=11]{isqrt_recursive.py}

Each step of the algorithm involves three big-integer shifts, one
big-integer addition, one bit-length computation, and one big-integer division,
along with a handful of operations that only involve small integers.

We can improve this slightly: one of the two right-shifts can be eliminated, by
keeping track of the amount by which~$n$ should be shifted in the recursive
call instead of actually shifting. With a little more bookkeeping, we can
also replace the per-iteration bit-length computation with a single
initial bit-length computation. These changes represent minor efficiency
improvements at the expense of some small loss of clarity; we leave the
interested reader to pursue them further.

The \lstinline{math.isqrt} implementation in CPython 3.8 doesn't use the
recursive version shown in Listing~\ref{rec}. Instead, we unwind the recursion
to obtain an iterative version of the algorithm. This version is presented in
Listing~\ref{new_iterative}.

\lstinputlisting[
  float,
  frame=single,
  label=new_iterative,
  caption={Adaptive precision Heron's method, iterative},
  lastline=13]{isqrt_iterative.py}

It may not be immediately obvious that Listing~\ref{new_iterative} is
equivalent to Listing~\ref{rec}. Rather than describing the equivalence and
establishing the correctness of the iterative version via that equivalence,
it's simpler to give a direct proof of correctness for the iterative version.

We establish two loop invariants on the variables $s$, $d$ and $a$. These
invariants hold just before entering the \lstinline$while$ loop, and at the end
of every iteration of that loop (and hence also the beginning of any subsequent
iteration). The first loop invariant is $d = \floor{c/2^s}$; this should be
clear from examining the code. The second loop invariant states that at every
step, $a$ is a near square root of $\floor{n / 4^{c-d}}$. In particular, on exit
from the \lstinline$while$ loop, $s = 0$, $d = \floor{c/2^0} = c$ and so $a$ is
a near square root of $\floor{n / 4^{c-c}} = n$.

To establish the second invariant, note first that the invariant holds just
before we reach the \lstinline$while$ loop: we have $c = \floor{\log_4 n}$, so
$4^c \le n < 4^{c+1}$ and $1 \le \floor{n / 4^c} < 4$, so $a = 1$ is a near
square root of $\floor{n / 4^c}$. We then need to establish that if the
invariant holds at the beginning of any while loop iteration, it also applies
at the end of that iteration. We prove this via the following lemma, in which
$b$ and $e$ represent the values of $a$ and $d$ at the start of the iteration.

\begin{lemma}
  Suppose that $0 \le s < \length(c)$, that $e = \floor{c / 2^{s + 1}}$, that $d =
  \floor{c / 2^s}$, and that $b$ is a near square root of $\floor{n /
  4^{c-e}}$. Let
  $$a = 2^{d-e-1}b + \floor*{\frac{n}{2^{2c-d-e+1}b}}.$$ Then $a$ is a near
  square root of $\floor{n / 4^{c-d}}$.
\end{lemma}

\begin{proof}
  Let $m = \floor{n / 4^{c-d}}$. Then $b$ is a near square root of
  $\floor{m/4^{d-e}}$ and we can rewrite $a$ as
  $$a = 2^{d-e-1}b + \floor*{\frac{m}{2^{d-e+1}b}}.$$ Now we can apply the main
  theorem: if we can show that $2^{d-e-1}$ is an integer and that
  $4(2^{d-e-1})^4 \le m$, it follows from Theorem~\ref{main_theorem} that $a$
  is a near square root of $m$. But from the definitions of $d$ and $e$, $1 \le
  d$ and $e = \floor{d/2}$, so it follows that $0 \le d - e - 1 \le e$, and
  \begin{align*}
    4(2^{d-e-1})^4 &= 4(4^{d-e-1})^2 \\
                  &= 4\cdot 4^{d-e-1}\cdot 4^{d-e-1} \\
                  &\le 4\cdot 4^{d-e-1}\cdot 4^e \\
                  &= 4^d
  \end{align*}
  Furthermore, since $c = \floor{\log_4 n}$, $4^c \le n$, so $4^d \le n /
  4^{c-d}$, hence $4^d \le m$. Combining this with the inequality above,
  $4(2^{d-e-1})^4 \le m$, as required.
\end{proof}

The quantities $c$, $d$, $e$, and $s$ are all small (machine-size)
integers; only $a$ and $n$ are big integers. So the algorithm consists of
two big-integer shifts, one big-integer addition and one big-integer division per
iteration, along with a handful of operations with small integers.

The total number of iterations $m$ of the while loop turns out to be remarkably
simple: it's exactly $\floor*{\log_2 \floor{\log_2 n}}$, assuming $n\ge 2$ (and zero
iterations are required for $n = 1$). So
for example, input values $n$ satisfying $2^{32} \le n < 2^{64}$ require exactly
five iterations, while values in the range $2^{64} \le n < 2^{128}$ require
exactly six.

\section{Fixed-precision algorithms}

The iterative algorithm shown in Listing~\ref{new_iterative} specialises easily
to particular fixed-precision cases, giving an almost branch-free algorithm.
For example, if $2^{30} \le n < 2^{32}$, then $c = 15$, $s=4$, and we can
compute in advance the set of $d$ and $e$ values for each of the four
iterations. For smaller $n$, we can find an $f$ such that $2^{30} \le 4^f n <
2^{32}$, apply the same algorithm to $4^f n$, then shift the resulting near
square root right by~$f$ bits. This gives the algorithm shown in
Listing~\ref{isqrt32}. The corresponding 64-bit version is shown in
Listing~\ref{isqrt64}. While the original iterative algorithm was developed
for positive $n$, these fixed-precision variants also turn out
to give the correct answer in the case $n = 0$.

\lstinputlisting[
  float,
  frame=single,
  label=isqrt32,
  caption={Fixed-precision variant, valid for $0 \le n < 2^{32}$},
  lastline=10]{isqrt32.py}

\lstinputlisting[
  float,
  frame=single,
  label=isqrt64,
  caption={Fixed-precision variant, valid for $0 \le n < 2^{64}$},
  lastline=10]{isqrt64.py}


\section{Using floating-point}

If we have access to a floating-point type that provides IEEE 754-2008
floating-point format and semantics, we can use floating-point arithmetic to
compute integer square roots directly for small~$n$, and to accelerate the
computation of integer square roots for larger~$n$. Note that Python does not
currently require IEEE 754 format or semantics, but that in practice use of
IEEE 754 floating-point is almost ubiquitous on platforms that support Python,
and Python's \lstinline{float} type is highly likely to map to the IEEE
754-2008 binary64 ``double precision'' format.

In this section, we assume that Python's \lstinline{float} type does use
the binary64 format, and that basic arithmetic operations and the standard
library \lstinline{math.sqrt} function are all correctly rounded, using the
default IEEE 754 round-ties-to-even rounding mode.

First some definitions: given a real number~$x$, write $\float(x)$ for the
nearest binary64 floating-point number to~$x$, following the IEEE 754 rules
with the default ``roundTiesToEven" rounding rule. (If $x$ has magnitude
$2^{1024} - 2^{970}$ or greater, $\float(x)$ is the appropriately-signed
infinity.) If $x$ is already a finite floating-point number, we implicitly
regard it as a real number when necessary. Given a finite nonnegative
binary64 floating-point number $x$, write $\fsqrt(x)$ for the correctly-rounded
square root of~$x$, and $\fsqr(x)$ for the correctly-rounded square of $x$. In
other words, $\fsqrt(x) = \float(\sqrt x)$ and $\fsqr(x) = \float(x^2)$.

We first prove the statement givenn in the introduction, that if $n$ is
not too large, $\floor{\fsqrt(n)}$ gives the integer square root of~$n$.

\begin{lemma}
  Suppose that $n$ is an integer satisfying $1 \le n < 2^{52}$. Then
  $\floor{\fsqrt(n)}$ is the integer square root of~$n$.
\end{lemma}

\begin{proof}
  Write $a$ for the integer square root of $n$. Then $a^2 \le n < (a + 1)^2$,
  so $a \le \sqrt n < a + 1$, and since both $a$ and $a + 1$ are easily small
  enough to be exactly representable as floats, it follows that
  $$a \le \float(\sqrt n) \le a + 1.$$
  Note that we have a $\le$ in the right-hand inequality, because there's a
  possibility that $\float$ rounded $\sqrt n$ up to the next integer. To prove
  the lemma, we need to eliminate this possibility.

  Choose an integer $k$ such that $2^k \le a < 2^{k + 1}$. Floats in
  the interval $[2^k, 2^{k+1}]$ are spaced by $2^{k-52}$, so the next
  float down from $a + 1$ is $a + 1 - 2^{k - 52}$, and it's enough for
  us to show that
  $$\sqrt n < a + 1 - 2^{k-53}$$
  or equivalently that
  $$a + 1 - \sqrt n > 2^{k - 53}.$$
  But we have
  $$a + 1 - \sqrt n = \frac{(a + 1)^2 - n}{a + 1 + \sqrt n}.$$
  The numerator is at least~$1$, and since $\sqrt n < a + 1$, the denominator is less
  than $2(a+1)$, which is in turn less than or equal to $2^{k+2}$. So
  $$a + 1 - \sqrt n > 2^{-k-2}.$$
  So we only need to show that $2^{-k-2} \ge 2^{k-53}$. But that's equivalent
  to $2k \le 51$, and from our assumptions, $a^2 \le n < 2^{52}$, so
  $2^k \le a < 2^{26}$, hence $k < 26$.
\end{proof}


\begin{lemma}
  \label{float-scale}
  Suppose that $x$ and $y$ are positive real numbers such that $x = 2^e y$ for
  some integer $e$, and that both $x$ and $y$ lie in the half-open interval
  $[2^{-1022} - 2^{-1076}, 2^{1024} - 2^{970})$. Then $\float(x) = 2^e \float(y)$.
\end{lemma}

\begin{proof}
  The bounds on $x$ and~$y$ ensure that $\float(x) = 2^f \rint(x / 2^f)$, where
  $f = \floor{\log_2 x} - 52$, and similarly for~$y$. Here $\rint(x)$ is the
  operation that rounds a real number to the nearest integer, rounding halfway
  cases to the even integer. The result follows directly from this.
\end{proof}

The following fact is well known, but we state and prove it here for convenience.

\begin{lemma}
  Suppose $x$ is an IEEE 754 binary64 floating-point number satisfying
  $2^{-511} \le x < 2^{512}$. Then $\fsqrt(\fsqr(x)) = x$.
\end{lemma}

\begin{proof}
  We first use the previous lemma to reduce to the case where $1/2 < x \le 1$.
  Choose an integer $e$ and floating-point number $y$ such that $1/2 < y \le 1$
  and
  $$x = 2^e y.$$
  Then
  $$x^2 = 2^{2e}y^2$$
  so applying Lemma~\ref{float-scale},
  $$\float(x^2) = 2^{2e}\float(y^2).$$
  By definition of $\fsqr$, this can be rewritten as
  $$\fsqr(x) = 2^{2e}\fsqr(y).$$
  Now taking square roots of both sides,
  $$\sqrt{\fsqr(x)} = 2^e \sqrt{\fsqr(y)}.$$
  Applying Lemma~\ref{float-scale} again,
  $$\float(\sqrt{\fsqr(x)}) = 2^e \float(\sqrt{\fsqr(y)}),$$
  or in other words
  $$\fsqrt(\fsqr(x)) = 2^e \fsqrt(\fsqr(y)).$$
  So to prove that $\fsqrt(\fsqr(x)) = x$, it's enough to prove that
  $\fsqrt(\fsqr(y)) = y$, since we then have
  $$\fsqrt(\fsqr(x)) = 2^e \fsqrt(\fsqr(y)) = 2^e y = x.$$
  Or in other words, it's enough to prove the original statement in the special case that
  $1/2 < x \le 1$. So from this point on, we assume that $1/2 < x \le 1$.

  Let $z = \fsqr(x)$ be the closest float to $x^2$. Then $1/4 \le z \le 1$ and
  since floats are spaced at most $2^{-53}$ apart within the interval $[1/4,
  1]$,
  $$\abs{z - x^2} \le 2^{-54}.$$ Now we have
  $$\sqrt z - x = \frac{(\sqrt z - x)(\sqrt z + x)}{\sqrt z + x} = \frac{z -
  x^2}{\sqrt z + x}$$ and since $1/2 < x$ and $1/2 \le \sqrt z $, $\sqrt z + x
  > 1$. It follows that
  $$\abs{\sqrt z - x} < 2^{-54}$$ and so since floats in the interval $[1/2,
  1]$ are spaced exactly $2^{-53}$ apart, it follows that $x$ is the unique
  closest float to $\sqrt z$, so $x = \fsqrt(z)$ as required.
\end{proof}

The next corollary follows immediately from the lemma, together with the
fact that any nonnegative integer not exceeding $2^{53}$ can be exactly
represented in binary64 floating-point.

\begin{corollary}
  Suppose $n$ is a nonnegative integer satisfying $n \le 2^{53}$. Then
  $\fsqrt(\fsqr(n)) = n$.
\end{corollary}

\begin{corollary}
  Suppose $n$ is a positive integer satisfying $n \le 2^{106}$. Then
  $\floor{\fsqrt(\rnd(n))}$ is a near square root of $n$.
\end{corollary}

\begin{proof}
  First suppose that $n$ is a perfect square: $n = a^2$. Then $\rnd(n) =
  \fsqr(a)$, so from the previous corollary, $\fsqrt(\rnd(n)) =
  \fsqrt(\fsqr(a)) = a$.

  Now suppose that $n$ is not a perfect square, so that $a^2 < n < (a+1)^2$
  for some nonnegative integer $a < 2^{53}$. Then $\rnd$ is monotonic, so
    $$\rnd(a^2) \le \rnd(n) \le \rnd((a+1)^2)$$
  or equivalently,
    $$\fsqr(a) \le \rnd(n) \le \fsqr(a+1).$$
  Similarly, $\fsqrt$ is monotonic, so applying $\fsqrt$ throughout
  and using the previous corollary,
    $$a \le \fsqrt(n) \le a + 1.$$
  Hence $\floor{\fsqrt(\rnd n)}$ is either $a$ or $a+1$, so $a$ is a near
  square root of $n$.
\end{proof}

In fact, $\floor{\fsqrt(\rnd n)}$ gives us a near square root for all
$n \le 2^{106} + 2^{54}$. For $n = 2^{106} + 2^{54} + 1 = (2^{53} + 1)^2$,
$\rnd n = 2^{106} + 2^{54}$ and $\fsqrt(\rnd n) = 2^{53}$.

% \section{To do}

% - Note that if a value is already known to be a square, no check-and-correct
% step is needed.
% - Similarly for square detection: take a near square root, square it, and check.
% - And square root ceiling is easy, too.
% - Running time analysis, and real-world timings to back it up. Asymptotically,
% isqrt of a $2n$-bit integer should be faster than division of a $2n$-bit integer
% by an $n$-bit dividend.
% - Fixed-precision versions.
% - Floating-point methods and floating-point accelerations.
% - Details on correctness of floating-point algorithm.
% - Reference(s) for "Many number-theoretic and cryptographic algorithms ..." Cohen?
% - Reference for Java BigInteger.sqrt
% - Reference for Heron's method / Babylonian method / Newton's method.
% - Reference for assertion that the number of correct decimal places
% doubles with each iteration.

\end{document}
