\documentclass[a4paper]{article}

\usepackage{listings}
\usepackage{amsthm}
\usepackage{mathtools}

\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclareMathOperator{\length}{length}
\DeclareMathOperator{\rnd}{rnd}
\DeclareMathOperator{\fsqrt}{fsqrt}
\DeclareMathOperator{\fsqr}{fsqr}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\title{Python's integer square root algorithm}
\author{Mark Dickinson}

\begin{document}
\lstset{language=Python}
\maketitle
\begin{abstract}
We present an adaptive-precision variant of the Babylonian method for computing
integer square roots of arbitrary-precision integers. The method is efficient,
both at small scales and asymptotically, and represents an attractive
compromise between speed and simplicity. The algorithm is used by the CPython
implementation of the Python programming language for its standard library
integer square root function.
\end{abstract}
\section{Introduction}

We start with a simple definition.

\begin{definition}
  For a nonnegative integer $n$, the \emph{integer square root} of $n$ is
  the unique nonnegative integer $a$ satifying $a^2 \le n < (a + 1)^2$.
\end{definition}

Equivalently, the integer square root of $n$ is simply the integer part
of the exact square root of $n$, or $\floor{\sqrt n}$.

The integer square root is a basic building block of any arbitrary-precision
arithmetic toolkit. Many number-theoretic algorithms require the ability
to detect whether a given integer is a square, and if so, to extract its
root. (XXX Ref: Cohen.)

For small integers $n$, it's feasible to use floating-point arithmetic to
compute square roots. For example, assuming IEEE 754 binary64 format
floating-point and a correctly-rounded square root operation, one can
show that computing $\floor{\sqrt n}$ directly gives the integer square root of
$n$, provided that $n < 2^{52} + 2^{27}$. (XXX give more details in a later
section.)

However, Python guarantees neither IEEE 754 format floating-point nor correct
rounding of floating-point arithmetic operations. As such, any
floating-point-based method for computing an integer square root would need a
pure-integer fallback method for the case where the floating-point sqrt
fails to provide sufficient accuracy. To avoid this complication, and to allow
integer square roots to be computed for arbitrarily large integers, it's desirable
to have an algorithm that works entirely with integer arithmetic, avoiding
floating-point.

In this paper we present a simple integer-only algorithm to compute the integer
square root of an arbitrary nonnegative number. This algorithm has been
implemented for CPython's \lstinline{math} module and is available in Python
3.8 as \lstinline{math.isqrt}.

\section{Heron's method}

In this section we describe a well-known approach to computing integer square
roots, based on an integer arithmetic version of Heron's method (also known
as the Babylonian method, and expressible as a special case of the
Newton-Raphson root-finding method. XXX give reference). This is the approach
used for example by Java's BigInteger class. (XXX needs reference.)

Heron's method is based on the observation that if $x$ is an approximation to
the (real) square root of an input $n$, then
$$\frac{x + n/x}2$$ is an improved approximation. Iterating then allows the
square root to be computed to any desired accuracy. It can be shown that the
method converges towards the square root from any positive starting
approximation $x$, and that once $x$ gets close to the true square root the
number of correct decimal places roughly doubles with each iteration. (XXX
needs reference.)

\begin{example}
  Given an approximation $x = 7/5 = 1.4$ to the square root $1.414213562\dots$
  of $2$, a single iteration of Heron's method produces a new approximation
  $(7/5 + 2/(7/5))/2 = 99/70 = 1.414285714\dots$. Applying a second iteration
  with $99/70$ as input gives $19601/13860 = 1.414213564\dots$, which is
  accurate to $8$ decimal places. A third iteration gives a value accurate to
  $17$ decimal places.
\end{example}

Heron's method can be adapted to the domain of positive integers. For a fixed
positive integer $n$, define a function $f$ on the positive integers by
$$f(a) = \floor*{\frac{a + \floor{n/a}}2}.$$

The idea is that, just like its real analogue, the function $f$ should
transform a poor approximation to the integer square root of $n$ into a better
one, and so by applying $f$ repeatedly we should eventually reach the integer
square root. This works in practice, but we have to be a little careful with
the details: in particular, correct termination of the algorithm is delicate.
The lemmas below make this precise.

\begin{lemma}
  For any positive integer $a$, $f(a)$ is greater than or equal to the
  integer square root of $n$.
\end{lemma}

\begin{proof}
  The AM-GM inequality applied to $a$ and $n/a$ gives
  $$\sqrt n \le \frac{a + n/a}2.$$
  Hence
  $$\floor{\sqrt n} \le \frac{a + n/a}2.$$
  Rearranging gives
  $$2\floor{\sqrt n} -a \le n/a,$$
  which implies
  $$2\floor{\sqrt n} - a \le \floor{n/a}.$$
  Rearranging again gives
  $$\floor{\sqrt n} \le \frac{a + \floor{n/a}}2$$
  and so
  $$\floor{\sqrt n} \le \floor*{\frac{a + \floor{n/a}}2}$$
  which is the desired result.
\end{proof}

\begin{lemma}
  Given a positive integer $a$ that's greater than the integer square
  root of $n$, $f(a)$ is smaller than $a$.
\end{lemma}

\begin{proof}
  We have the following chain of equivalences:
  \begin{align}
    \floor{\sqrt n} < a &\iff \sqrt n < a \\
                        &\iff n < a^2 \\
                        &\iff n/a < a \\
                        &\iff \floor{n/a} < a \\
                        &\iff a + \floor{n/a} < 2a \\
                        &\iff \frac{a + \floor{n/a}}2 < a \\
                        &\iff \floor*{\frac{a + \floor{n/a}}2} < a \\
                        &\iff f(a) < a.
  \end{align}
  This completes the proof.
\end{proof}

Now suppose that we're given a starting guess $a$ that exceeds the integer
square root of $n$: $\floor{\sqrt n} < a$. Combining the above lemmas,
we have
$$\floor{\sqrt n} \le f(a) < a.$$ If $f(a)$ is again greater than the integer
square root of $n$, we have:
$$\floor{\sqrt n} \le f(f(a)) < f(a) < a$$ and so on. So the sequence
$$a, f(a), f(f(a)), f(f(f(a))), \dots$$
must eventually reach the integer square root. If not, we'd have an infinite
bounded-below strictly descreasing sequence of integers, which is impossible.

Moreover, the second lemma also provides a way to detect when we've reached the
integer square root: if $\floor{\sqrt n} \le a$ but the inequality $f(a) < a$
fails, then $a$ cannot be \emph{greater} than the integer square root, so it
must \emph{be} the integer square root.

While the sequence
$$a, f(a), f(f(a)), \dots$$ must eventually \emph{reach} the square root, it
would be a mistake to say that it \emph{converges} to the square root: if $a$
is the integer square root of $n$, then it's possible to have $f(a) > a$. For
example, if $n=15$, the sequence above will eventually alternate between $3$
and $4$, and more generally this happens for any $n$ of the form $m^2 - 1$
for some $m \ge 2$.

From a computational perspective, an expression like $\floor{n / a}$
is misleading: it resembles a composition of two operations, with the
intermediate result living in the domain of rational numbers. But in reality
most programming languages or arbitrary-precision integer arithmetic packages
will provide integer division (or floor division) as a single
integer-to-integer primitive. In Python, for example, we'd write the expression
defining $f(a)$ as \lstinline$(a + n//a) // 2$, where the \lstinline$//$ operator
is Python's ``floor division" operator. Here's the full algorithm translated directly
to Python.

\lstinputlisting[lastline=12]{isqrt_newton_direct.py}

Here's a more natural and streamlined version of the algorithm. The termination
condition $f(a) \ge a$ is replaced with the equivalent condition $\floor{n/a}
\ge a$. For our starting guess, we take the smallest power of two that exceeds
$\sqrt n$.  (In Python, \lstinline$n.bit_length()$ gives the smallest
nonnegative integer $k$ for which $n < 2^k$.)

\lstinputlisting[lastline=7]{isqrt_newton.py}

The algorithm outlined above has been fairly widely adopted. It has the virtue
of simplicity, and is reasonably efficient for inputs that aren't too large.
Nevertheless, it's not perfect. The choice of initial guess is delicate, and a
poor choice will have a significant effect on running time. For large inputs
(say a few thousand bits or more), the first few iterations of the algorithm
are performing expensive full-precision divisions to obtain only a handful of
new correct bits at each iteration. And there's potential inefficiency towards
the end of the algorithm, too. To demonstrate this, consider the following
example.

\begin{example}
  Take $n = 16785408$, which is just a little larger than $2^{24} = 16777216$.
  Our starting guess for the square root is $2^{13} = 8192$. Successive
  iterations give $5120$, $4199$, $4098$, $4097$, and finally $4096$, which
  is the integer square root. Each iteration requires one division, and
  a final division is needed to establish the termination condition, for
  a total of six divisions.
\end{example}

So even starting from $4098$, just a distance of two away from the true integer
square root, three more divisions are required before the correct integer
square root can be returned.

The algorithm introduced in the next section addresses these deficiencies,
while retaining much of the simplicity of the algorithm in this section.

\section{Variable-precision Heron's method}

In this section we introduce a simple variant of Heron's method that's
significantly more efficient than the basic algorithm for large inputs.

As in previous sections, we assume that $n$ is a positive integer, and we
aim to compute the integer square root of $n$.

There are two key ideas. First, we vary the precision as we go: the algorithm
produces at each iteration an approximation to the square root of
$\floor{n/4^s}$ for some integer $s$, with $s$ decreasing as the iterations
progress. Second, we don't insist on obtaining the exact integer square root at
each iteration (which would require a per-iteration check-and-correct
operation), but instead allow the error to propagate, and we prove
that with careful control of the rate at which the precision
increases, the error remains bounded throughout the algorithm. We then need a
final check-and-correct step at the end of the algorithm.

To describe the algorithm, it's convenient to introduce the notion of a
``near square root".

\begin{definition}
  Suppose $n$ is a positive integer. Call a positive integer $a$ a
  \emph{near square root} of $n$ if $(a - 1)^2 < n < (a + 1)^2$.
\end{definition}

In other words, $a$ is a near square root of $n$ if $a$ is either $\floor{\sqrt
n}$ or $\ceil{\sqrt n}$. In particular, if $n = a^2$ is a perfect square then
the only near square root of $n$ is $a$. Given a near square root $a$ of $n$,
the integer square root of $n$ is clearly either $a$ or $a-1$, depending on
whether $a^2 \le n$ or $a^2 > n$ (respectively). So an algorithm for computing
near square roots provides us with a way to compute integer square roots.

Here's the idea of the algorithm: given a near square root $d$ of $\floor{n /
k^2}$ for some positive integer $k$, $dk$ is then an approximation to $\sqrt
n$, although possibly not a very good one. A single iteration of Heron's method
applied to $dk$ gives an improved approximation. Now comes the key point: if
we're careful not to choose $k$ too large with respect to $n$, we can prove
that this improved approximation will again be a near square root of $n$.

The following theorem makes this precise. To keep calculations within the domain
of the integers, it's convenient to restrict $k$ to be even. So in the theorem
below, $k$ is replaced with $2m$.

\begin{theorem}
  Suppose that $n \ge 4$, and choose a positive integer $m$
  satisfying $4m^4 \le n$. Given a near square root $d$ of $\floor{n / 4m^2}$,
  define $a$ by
  $$ a = md + \floor*{\frac{n}{4md}}. $$
  Then $a$
  is a near square root of $n$.
\end{theorem}

In the notation of the previous section, $a$ is defined by $a = f(2md)$.

\begin{proof}
  By definition of near square root, we have
  $$ (d - 1)^2 < \floor*{\frac{n}{4m^2}} < (d + 1)^2.$$
  Since $(d + 1)^2$ is an integer, we can remove the floor brackets to obtain
  $$ (d - 1)^2 < \frac{n}{4m^2} < (d + 1)^2.$$
  Taking square roots throughout and multiplying by $2m$ gives
  $$ 2m(d - 1) < \sqrt n < 2m(d + 1)$$
  which can be rearranged as
  $$ \abs{2md - \sqrt n} < 2m. $$
  Squaring and dividing through by $4md$ gives
  $$ 0 \le md + \frac{n}{4md} - \sqrt n < \frac md,$$
  which implies that
  $$ -1 < md + \floor*{\frac{n}{4md}} - \sqrt n < \frac{m}{d},$$
  Substituting the definition of $a$ gives
  $$ -1 < a - \sqrt n < m / d.$$
  To complete the proof, we need to know that $m \le d$, and so $m / d \le 1$.
  From the assumption that $4m^4 \le n$ we have $m^2 \le n / 4m^2 < (d
  + 1)^2$, so $m < d + 1$. So now
  $$ -1 < a - \sqrt n < 1 $$
  from which $(a - 1)^2 < n < (a + 1)^2$, as required.
\end{proof}

Now we turn to implementation. Like many arbitrary-precision integer
implementations, Python's integer implementation is based on binary, so
multiplications and divisions by powers of two can be performed efficiently via
bit shifts. For maximum efficiency when applying the lemma above, we
choose our $m$ to be the largest power of two satisfying $4m^4 \le n$. It's
convenient to introduce another definition.

\begin{definition}
  For a nonnegative integer $n$, the \emph{bit length} of $n$, written
  $\length(n)$, is the least nonnegative integer $e$ for which $n < 2^e$.
\end{definition}

Equivalently, for positive $n$ the bit length of $n$ is $1 +
\floor{\log_2(n)}$, while the bit length of $0$ is $0$. For nonnegative
integers $n$ and $e$, we have $2^e \le n$ if and only if $e < \length(n)$.

When $m = 2^e$ is a power of two, the condition $4m^4 \le n$ of the lemma
becomes $2^{2 + 4e} \le n$. That's equivalent to $2 + 4e < \length(n)$, or $e
\le (\length(n) - 3) / 4$. Taking $e = \floor{(\length(n) - 3) / 4}$ gives the
following recursive near square root implementation, where the multiplication
by $m$ and the divisions by $4m$ and $4m^2$ are replaced by the corresponding
bit-shift operations.

\lstinputlisting[lastline=6]{isqrt_recursive.py}

And that's it! Just six lines of simple Python code are needed to compute near
square roots efficiently. Each step involves three big-integer shifts, one
big-integer addition, one bit-length computation, and one big-integer division,
along with a handful of operations that only involve small integers.

We can improve this slightly: one of the two right-shifts can be eliminated, by
keeping track of the amount by which~$n$ should be shifted in the recursive
call instead of actually shifting. With a little more bookkeeping, we can
also replace the per-iteration bit-length computation with a single
initial bit-length computation. Below we show an iterative version of the
algorithm that makes both these improvements, and also adds the final
check-and-correct step needed to return the integer square root rather
than just a near square root. This version is closer to the actual
CPython 3.8 implementation.

\lstinputlisting[lastline=10]{isqrt_iterative.py}

To understand the translation to the iterative version, it's helpful to look at
how the quantity $c = \floor{\log_4 n}$ changes over the course of the
recursive algorithm. We can express $c$ in terms of the bit-length of $n$ as $c
= \floor{(\length(n) - 1) / 2}$. Then $e = \floor{(c-1)/2}$, so $e+1 =
\floor{(c+1)/2} = \ceil{c/2}$. So
$\floor*{\log_4\floor{n/4^{e+1)}}} = \floor{\log_4 n} - {e + 1}$,
which is equal to $c - \ceil{c/2} = \floor{c/2}$.

So the sequence of values for $\floor{\log_4 n}$ as the recursion progresses
is
$$c, \floor{c/2}, \floor{c/4}, \floor{c/8}, \dots, \floor{c/2^m} = 0,$$
where $m$ is the least nonnegative integer for which $c < 2^m$, or in
other words, the bit-length of $c$.

In the iterative version of the algorithm, the variables $d$ and $e$ correspond
to successive pairs in the above sequence, progressing from right to left. We
start with $d=\floor{c/2^{m-1}}$ and $e=\floor{c/2^m}=0$ on the first
iteration, and end with $d=\floor{c/2^0}=c$ and $e=\floor{c/2^1}$ on the final
iteration. Each iteration starts with $a$ a near square root of $\floor{n
/4^{c-e}}$ and transforms that $a$ into a near square root of $\floor{n /
4^{c-d}}$. In particular, the first iteration needs to start with a near square root of
$\floor{n/4^c}$, but we have $1 \le \floor{n/4^c} < 4$, and so $a = 1$ is
a suitable starting value. And after the final iteration, $a$ is a near square root
of $\floor{n/4^{c-c}} = n$, as required.

Note that the quantities $c$, $d$, $e$, $m$ and $s$ are all small (machine-size)
integers. Only $a$ and $n$ are big integers. So the algorithm consists of
two big-integer shifts, one big-integer addition and one big-integer division per
iteration, along with a handful of operations with small integers.

The total number of iterations $m$ is remarkably simple: it's exactly
$\floor{\log_2 \floor{\log_2 n}}$, assuming $n\ge 2$. So for example input
values $n$ satisfying $2^{32} \le n < 2^{64}$ require exactly $5$ iterations,
while values in the range $2^{64} \le n < 2^{128}$ require exactly $6$.

\section{Floating-point quick start}

If we can assume IEEE 754 floating-point and semantics, we can use
floating-point for low precision. In this section we assume IEEE 754
binary64 (``double precision") floating-point, and correctly-rounded
arithmetic operations including square root.

Write $\fsqrt(x)$ for the correctly-rounded square root of an IEEE 754 binary64
floating-point value $x$, and $\fsqr(x)$ for the correctly-rounded square
of $x$.

\begin{lemma}
  Suppose $x$ is a positive finite binary64 floating-point number, satisfying
  $2^{-511} \le x < 2^{512}$. Then $\fsqrt(\fsqr(x)) = x$.
\end{lemma}

\begin{proof}
  The bounds on $x$ ensure that overflow and underflow are avoided, so
  that $\fsqr(x)$ is again positive and normal.

  Provided that underflow and overflow are avoided, we have $\fsqrt(4^e x) =
  2^e \fsqrt(x)$ and $\fsqr(2^e x) = 4^e \fsqr(x)$. Using this, we can
  assume that $\frac 12 \le x < 1$.


\end{proof}

The next corollary follows immediately from the lemma, together with the
fact that any nonnegative integer not exceeding $2^{53}$ can be exactly
represented in binary64 floating-point.

\begin{corollary}
  Suppose $n$ is a nonnegative integer satisfying $n \le 2^{53}$. Then
  $\fsqrt(\fsqr(n)) = n$.
\end{corollary}

\begin{corollary}
  Suppose $n$ is a positive integer satisfying $n \le 2^{106}$. Then
  $\floor{\fsqrt(\rnd(n))}$ is a near square root of $n$.
\end{corollary}

\begin{proof}
  First suppose that $n$ is a perfect square: $n = a^2$. Then $\rnd(n) =
  \fsqr(a)$, so from the previous corollary, $\fsqrt(\rnd(n)) =
  \fsqrt(\fsqr(a)) = a$.

  Now suppose that $n$ is not a perfect square, so that $a^2 < n < (a+1)^2$
  for some nonnegative integer $a < 2^{53}$. Then $\rnd$ is monotonic, so
    $$\rnd(a^2) \le \rnd(n) \le \rnd((a+1)^2)$$
  or equivalently,
    $$\fsqr(a) \le \rnd(n) \le \fsqr(a+1).$$
  Similarly, $\fsqrt$ is monotonic, so applying $\fsqrt$ throughout
  and using the previous corollary,
    $$a \le \fsqrt(n) \le a + 1.$$
  Hence $\floor{\fsqrt(\rnd n)}$ is either $a$ or $a+1$, so $a$ is a near
  square root of $n$.
\end{proof}

In fact, $\floor{\fsqrt(\rnd n)}$ gives us a near square root for all
$n \le 2^{106} + 2^{54}$. For $n = 2^{106} + 2^{54} + 1 = (2^{53} + 1)^2$,
$\rnd n = 2^{106} + 2^{54}$ and $\fsqrt(\rnd n) = 2^{53}$.

\section{To do}

- Note that if a value is already known to be a square, no check-and-correct
step is needed.
- Similarly for square detection: take a near square root, square it, and check.
- And square root ceiling is easy, too.
- Running time analysis, and real-world timings to back it up. Asymptotically,
isqrt of a $2n$-bit integer should be faster than division of a $2n$-bit integer
by an $n$-bit dividend.
- Fixed-precision versions.
- Floating-point methods and floating-point accelerations.
- Details on correctness of floating-point algorithm.

\end{document}
